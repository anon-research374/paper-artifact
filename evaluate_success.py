

import json
import numpy as np
import math
import time
import os
from transformers import AutoTokenizer, GPT2Tokenizer
from sentence_transformers import SentenceTransformer
from nltk.tokenize import sent_tokenize
import torch
import argparse
import traceback
from sent_to_code.sent_to_code import initialize_resources, sent_to_code


parser = argparse.ArgumentParser(description="Decoding data generated by non-integer multiple parameters using an approximate method with data truncation.")
parser.add_argument('--i', type=str, required=True, help="Path to the JSONL data file to be processed.")
parser.add_argument('--bit-num', type=int, default=4, help="Each sentence represents the number of bits (must be consistent with the generation time).")
parser.add_argument('--h', type=int, default=6, help="STC matrix height (must be consistent with the generation time).")
parser.add_argument('--seg', type=int, required=True, help="Segment length (seg) used when generating data.")
args = parser.parse_args()

FILE_PATH = args.i
BIT_LENGTH = args.bit_num
MAT_HEIGHT = args.h
SEG_LENGTH = args.seg
CC_PATH = "./sent_to_code/data/4_kmeans/cc.pt"
EMBEDDER_PATH = "./sent_to_code/SemStamp-c4-sbert"
STC_MATRIX_PATH = './STC_code/stc_matrix.npy'
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'



def get_matrix(width, height):
    if 2 <= width <= 20 and 7 <= height <= 12 and os.path.exists(STC_MATRIX_PATH):
        matrices = np.load(STC_MATRIX_PATH)
        start = (height - 7) * 400 + (width - 1) * 20
        return matrices[start:start + width]
    else:
        if (1 << (height - 2)) < width:
            raise ValueError("Cannot generate matrix for this payload. Choose a higher constraint height.")
        np.random.seed(1)
        mask = (1 << (height - 2)) - 1
        bop = (1 << (height - 1)) + 1
        cols = []
        for i in range(width):
            while True:
                r = ((np.random.randint(1, mask + 1) & mask) << 1) + bop
                if r not in cols:
                    cols.append(r)
                    break
        return np.array(cols, dtype=np.uint32)


def arrange_matrices(shorter, longer, msg_length, inv_alpha):
    mat_type = np.zeros(msg_length, dtype=np.uint8)
    mat_width = np.full(msg_length, shorter, dtype=np.uint32)
    for i in range(msg_length):
        if np.sum(mat_width[:i]) + longer <= (i + 1) * inv_alpha + 0.5:
            mat_type[i] = 1
            mat_width[i] = longer
    return mat_type, mat_width


def stc_extract(vector, alpha, msg_length, mat_height):
    inv_alpha = 1 / alpha
    assert inv_alpha >= 1, 'Message length cannot exceed vector length!'
    assert 4 <= mat_height <= 31, 'The height of the submatrix should be within the range [4, 31]!'
    shorter = math.floor(inv_alpha)
    longer = math.ceil(inv_alpha)
    columns = [get_matrix(shorter, mat_height), get_matrix(longer, mat_height)]
    binmat = [np.unpackbits(columns[0][..., np.newaxis].astype('>u4').view(np.uint8), axis=1)[:, -mat_height:][:, ::-1],
              np.unpackbits(columns[1][..., np.newaxis].astype('>u4').view(np.uint8), axis=1)[:, -mat_height:][:, ::-1]]
    mat_type, mat_width = arrange_matrices(shorter, longer, msg_length, inv_alpha)
    msg = np.zeros(msg_length, dtype=np.uint8)
    height = mat_height
    vec_idx = 0
    for msg_idx in range(msg_length):
        for k in range(mat_width[msg_idx]):
            if vec_idx < len(vector) and vector[vec_idx]:
                msg[msg_idx:msg_idx + height] ^= binmat[mat_type[msg_idx]][k][:height]
            vec_idx += 1
        if msg_length - msg_idx <= mat_height:
            height -= 1
    return msg


def recover_bit(text: str, bit_num: int, device: str) -> list:
    stego_bit = []
    for sentence in sent_tokenize(text):
        sentence = sentence.strip()
        if not sentence: continue
        bitstring = sent_to_code(sentence, device, 0.01)
        if bitstring:
            stego_bit.extend(int(b) for b in bitstring)
    return stego_bit


def compare_message_accuracy(json_obj, bit_length, mat_height, seg_length, device):
    try:
        idx = json_obj.get('idx', 'N/A')
        generated_text = json_obj.get("generated_sentence")
        original_message_raw = json_obj.get("message")
        alpha = json_obj.get("alpha")
        msg_length = json_obj.get("msg_size")

        if not all([generated_text, original_message_raw, alpha is not None, msg_length is not None]):
            print(f"Object idx {idx}: Skipped, missing required field.")
            return False, False, 0, 0

        if isinstance(original_message_raw, str):
            original_message_list = json.loads(original_message_raw)
        else:
            original_message_list = original_message_raw

        original_message = np.array(original_message_list)

        num_full_segments = msg_length // seg_length
        truncated_msg_length = num_full_segments * seg_length

        if num_full_segments == 0:
            return False, False, 0, msg_length


        seg_num = int(seg_length / alpha / bit_length)
        num_sentences_to_keep = num_full_segments * seg_num

        original_message_truncated = original_message[:truncated_msg_length]

        all_sentences = sent_tokenize(generated_text)
        if len(all_sentences) < num_sentences_to_keep:
            num_sentences_to_keep = len(all_sentences)

        truncated_sentences = all_sentences[:num_sentences_to_keep]
        truncated_text = " ".join(truncated_sentences)

        time1 = time.time()
        vector = recover_bit(truncated_text, bit_length, device)

        extracted_message = stc_extract(np.array(vector), alpha, msg_length=truncated_msg_length, mat_height=mat_height)
        time2 = time.time()
        print(f'  [Performance] Extraction Time: {time2 - time1:.4f} seconds')

        total_bits_to_check = len(original_message_truncated)
        matching_bits = np.sum(
            original_message_truncated[:len(extracted_message)] == extracted_message[:len(original_message_truncated)])
        are_equal = (len(original_message_truncated) == len(extracted_message)) and (
                    matching_bits == total_bits_to_check)

        if are_equal:
            print(f"âœ… Object idx {idx}: The complete part of the message matches (Partial Match).")
            print(f"   Verified {matching_bits}/{msg_length} bits")
        else:
            print(f"âŒ Object idx {idx}: The complete part of the message is inconsistent.")
            print(f"  - Original message (truncated to {len(original_message_truncated)} bits): {original_message_truncated}")
            print(f"  Extracted message ({len(extracted_message)} bits): {extracted_message}")
            if total_bits_to_check > 0:
                bit_acc_percent = (matching_bits / total_bits_to_check) * 100
                print(f" - Partial bit accuracy: {matching_bits}/{total_bits_to_check} ({bit_acc_percent:.2f}% accuracy)")

        return are_equal, True, matching_bits, total_bits_to_check

    except Exception as e:
        print(f"An error occurred while processing object idx {json_obj.get('idx', 'N/A')}: {e}")
        traceback.print_exc()
        return False, False, 0, 0


def main():
    try:
        initialize_resources(
            cc_path=CC_PATH,
            embedder_path=EMBEDDER_PATH,
            bit_length=BIT_LENGTH
        )
    except Exception as e:
        print(f"Error: Resource initialization failed. Please check the path '{CC_PATH}' and '{EMBEDDER_PATH}'. Details: {e}")
        return

    identical_count, different_count, processed_count, error_count = 0, 0, 0, 0
    total_bits_processed, total_matching_bits = 0, 0

    print(f"\nStart processing file: '{FILE_PATH}'")
    print(f"Use parameters: bit-num={BIT_LENGTH}, mat-height={MAT_HEIGHT}, seg={SEG_LENGTH}")
    print("Note: This script uses data truncation method and will only verify the complete paragraph part of the message.")

    try:
        with open(FILE_PATH, 'r', encoding='utf-8') as file:
            for line_number, line in enumerate(file, 1):
                if not line.strip(): continue
                try:
                    json_obj = json.loads(line.strip())

                    are_equal, processed, matching, total = compare_message_accuracy(
                        json_obj, BIT_LENGTH, MAT_HEIGHT, SEG_LENGTH, DEVICE
                    )

                    if processed:
                        processed_count += 1
                        if are_equal:
                            identical_count += 1
                        else:
                            different_count += 1
                        total_matching_bits += matching
                        total_bits_processed += total
                    else:
                        error_count += 1

                except json.JSONDecodeError:
                    error_count += 1

        print("\n========== Evaluation Summary (Based on Truncated Data) ==========")
        print(f"Total number of analyzed objects:{processed_count}")
        print(f"Number of failed or skipped objects processed:{error_count}")
        print(f"âœ… Number of consistent objects in the message section:{identical_count}")
        print(f"âŒ Number of inconsistent objects in the message section:{different_count}")

        if processed_count > 0:
            accuracy = (identical_count / processed_count) * 100
            print(f"ðŸŽ¯(Partial Match Accuracy): {accuracy:.2f}%")
        if total_bits_processed > 0:
            bit_accuracy = (total_matching_bits / total_bits_processed) * 100
            print(
                f" (Partial Bit Accuracy): {bit_accuracy:.2f}% ({total_matching_bits}/{total_bits_processed})")
        print("==============================================")

    except FileNotFoundError:
        print(f"Error: File '{FILE_PATH}' not found. Please check the file path.")
    except Exception as e:
        print(f"An unexpected error occurred during the processing: {e}")


if __name__ == '__main__':
    main()
